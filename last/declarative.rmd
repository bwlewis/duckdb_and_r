# Declarative, Schmerative!

SQL is billed as a language of declarative data manipulation and analysis.
But even for some simple tasks, SQL really seems more imperative to me than
alternatives like R.  Consider the following quite simple example using a large
table that starts out like:

```{r, echo = FALSE}
source("fake.r")

set.seed(1)
N_GROUPS <- 10
MAX_LEN <- 1e3
make_believe <- list(lengths = sample(MAX_LEN, N_GROUPS, replace = TRUE),
                     values = paste(sample(names, N_GROUPS, replace = TRUE),
                                    sample(sectors, N_GROUPS, replace = TRUE)))
make_believe <- inverse.rle(structure(make_believe, class = "rle"))
N <- length(make_believe)
example <- data.frame(date = Sys.Date() - sample(365, N, replace = TRUE),
                      company =  sample(make_believe), value = runif(N), stringsAsFactors = TRUE)
knitr::kable(head(example, 6))
```
with many more, maybe millions more, rows like that. You can see a much
longer discussion of this example here:
https://bwlewis.github.io/duckdb_and_r/last/last.html .
Consider the following very simple task:

> For each company in the example table, return the most recent value.

How simple is that?


## The Dplyr/Tidy Data way

An R dplyr approach to performing the task hews closely to the simple statement
above. The solution is composed of a few short and very declarative
functions:

```{r, eval=FALSE}
example %>% group_by(company) %>% summarize(answer = last(value, order_by = date))
```
Wow, you can almost put a period at the end and read it like a sentence.


## Low-level imperative pseudocode

Now let's imagine we are working with a very low-level imperative programming
language. One approach might go something like:

```
groups = unique(company)
for each group g
  let i = integer index that reverse orders date in group g
  keep row associated with i = 1
end for
```

Or, if you felt like being less efficient about things you could make several
passes over the data like so:

```
groups = unique(company)
for each group g
  let row.i = integer index that reverse orders date in group g
end for
for each row in example table
  if row.i = 1 then keep row
end for
```

## SQL Oh My

As far as I can tell, a standard SQL approach to solving this task looks
remarkably like the latter, less efficient, pseudocode above except harder to
read. The gist is number each row using date order within group and then go
through and pick out rows with the right number:

```
SELECT company, value FROM
  (SELECT company, date_numeric, value, ROW_NUMBER() OVER
    (PARTITION BY company ORDER BY date_numeric DESC) AS i FROM example) AS cazart
  WHERE i = 1
```

What's worse, query optimizers for at least a few SQL database engines I tried
quite literally interpret the SQL code and actually loop over the data
multiple times. See 
https://bwlewis.github.io/duckdb_and_r/last/last.html
for representative, and disastrously bad, SQL database engine timings.

Even worse yet, apparently some database engines ask the user themselves to work
around inefficient implementations by using highly idiosyncratic tricks. See, for
instance illuminating discussions on this problem for PostgreSQL and MySQL:

* https://hakibenita.com/sql-group-by-first-last-value
* https://stackoverflow.com/questions/1313120/retrieving-the-last-record-in-each-group-mysql#1313293


## Which approach looks more declarative to you?

Yeah.

SQL can be really nice for simple, basic aggregation and filtering.  It's also
really good at imposing constraints on data.  But hey, I used to work at a
database company and I've seen SQL queries that would make Mike Stonebraker
smash his banjo like Pete Townshend. SQL can be a mess, is very had to compose,
and sometimes makes it difficult to express even seemingly simple objectives
like the one above.

My advice for most people and most problems is this: just use dplyr if you can.
Don't use SQL.  Try to think about your data munging problems the dplyr way.
For anything other than the most basic operations, I think dplyr is a better
through-out grammar of data manipulation than SQL.  And besides, dplyr can
usually front your database anyway.


## Look, dplyr isn't perfect either

Remember, nothing--not dplyr, SQL, certainly not R--is always a perfect
solution to every problem. Indeed, dplyr fails to figure out how to translate
even this simple problem for back-end databases.  The dplyr approach also
brings with it considerable complexity for some tasks--just look at the
(ridiculously) large number of functions in the dplyr package:
```{r}
suppressPackageStartupMessages(library(dplyr))
length(ls("package:dplyr"))
```
That's a lot of stuff to remember.
And, don't forget that
plain old base R often works *really* well in many cases, including for this
problem. Even SQL, despite my misgivings about it, has many admirable qualities
and the advantage that tons of folks already know it very well (surely way
better than I do) and use it productively.

For more thoughts and examples along these lines, see my notes on this topic
here:

https://bwlewis.github.io/duckdb_and_r


### Appendix: What's with that 'AS cazart' in the query?

Some, but not all, SQL database systems require that I label (alias) the
sub-query for reasons unknown to me. I'm not sure what the SQL ISO/IEC
9075-2:2016 standard says about this, and I don't really want to pay $250 to
find out.  For me, it's just an example of variation in SQL implementations
across database systems.

<br/><br/>
